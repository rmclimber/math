{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ca0f92-58e3-4f43-b6af-776dbb8542e6",
   "metadata": {},
   "source": [
    "## *Math For Machine Learning* \n",
    "### Chapter 3 (Analytic Geometry): Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf19ca00-79fa-43b6-971c-fad67b1bb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c847862-613c-4345-9b4e-50d434ac55fd",
   "metadata": {},
   "source": [
    "#### Exercise 3.5: \n",
    "We have a Euclidean vector space \\\\(\\mathbb{R}^5\\\\) with the dot product. Given subspace \\\\(U\\subseteq\\mathbb{R}^5\\\\) and \\\\(x\\in\\mathbb{R}^5\\\\) as defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad3ae314-0cf3-4fd6-bdd4-d54886a7ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array(\n",
    "    [\n",
    "        [0, 1, -3, -1],\n",
    "        [-1, -3, 4, -3],\n",
    "        [2, 1, 1, 5],\n",
    "        [0, -1, 2, 0], \n",
    "        [2, 2, 1, 7]\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = np.array([-1, -0, -1, 4, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce9058-ebb7-4cc8-a4ca-66ce0923f70f",
   "metadata": {},
   "source": [
    "a. *Determine the orthogonal projection \\\\(\\pi_U(x)\\\\) of \\\\(x\\\\) onto \\\\(U\\\\).* \\\n",
    "Step 1: since we do not know whether the columns are linearly-independent, much less orthonormal, we can compute the pseudoinverse of \\\\(U\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a28c613-c477-469d-b7f3-bdee178d96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Up = np.linalg.pinv(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cff2c7-11ab-4d6b-af86-13d0678bba3a",
   "metadata": {},
   "source": [
    "Step 2: use the pseudoinverse \\\\(U^+\\\\) to form the projection: \\\\(\\pi_U(x) = UU^+x\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8569cc52-b3e4-4c8c-aa19-66ae7f0de109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projection pi(x) onto U is: [-1.57142857  1.14285714 -0.42857143  0.57142857  1.        ]\n"
     ]
    }
   ],
   "source": [
    "pi_x = U @ Up @ x.T\n",
    "print(f\"The projection pi(x) onto U is: {pi_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d893d-278f-4765-85ac-161ccda39578",
   "metadata": {},
   "source": [
    "\n",
    "b. *Determine the distance \\\\(d(x, U)\\\\).* \\\n",
    "Step 1: because \\\\(\\pi_U(x)\\\\) is the closest point in \\\\(U\\\\) to \\\\(x\\\\), we first need the vector \\\\(v\\\\) from \\\\(\\pi_U(x)\\to x\\\\) with \\\\(v = x - \\pi_U(x)\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461a8665-8863-4185-b3ca-6091225eb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pi_x - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752d3d4-1e07-4c4b-927c-25e99717f6c2",
   "metadata": {},
   "source": [
    "Step 2: now we just need \\\\(d = \\lVert v\\rVert\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea1e5cb3-0b2c-4629-b420-23be99af952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of d(x, U): 3.703280399090206\n"
     ]
    }
   ],
   "source": [
    "d = np.linalg.norm(v)\n",
    "print(f\"The value of d(x, U): {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6568b23-5393-4ef1-943e-441fff9a0c72",
   "metadata": {},
   "source": [
    "#### Exercise 3.6:\n",
    "Consider \\\\(\\mathbb{R}^3\\\\) with the inner product: \\\\(\\langle x, y\\rangle := x^TBy\\\\). \\\\(B\\\\) follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf8be190-8e49-4720-a205-723d4ff510b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array(\n",
    "    [\n",
    "        [2, 1, 0],\n",
    "        [1, 2, -1],\n",
    "        [0, -1, 2]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b59491-d227-450b-870f-bc85f0babcf3",
   "metadata": {},
   "source": [
    "\\\\(e_1, e_2, e_3\\\\) are the standard, canonical basis for \\\\(\\mathbb{R}^3\\\\). \n",
    "\n",
    "a. *Determine the orthogonal projection \\\\(\\pi_U(e_2)\\\\) of \\\\(e_2\\\\) onto \\\\(U=\\text{span}[e_1, e_3]\\\\).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0cade42-2068-4105-b37f-23ee276e8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array(\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0],\n",
    "        [0, 1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5de58-796d-47be-ad03-23af55070bd8",
   "metadata": {},
   "source": [
    "We can see immediately that \\\\(B\\\\) is symmetric. Now we check for whether it is positive definite by verifying that its eigenvalues are all positive: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "759993f2-b4ea-41d2-a76e-4d9dedd81f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all eigenvalues are positive: True\n"
     ]
    }
   ],
   "source": [
    "eigenvals, _ = np.linalg.eig(B)\n",
    "all_pos = all(val > 0 for val in eigenvals)\n",
    "print(f\"all eigenvalues are positive: {all_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17d841-8c5a-4afa-821b-0d6b5c68c429",
   "metadata": {},
   "source": [
    "We also know that the columns of \\\\(U\\\\) are linearly independent. This means that \\\\(U^TBU\\\\) is invertible, giving us the projection matrix: \\\\(P_B = U(U^TBU)^{-1}U^TB\\\\), which will allow us to find the project point \\\\(p_{e_2} = P_Be_2\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8716b07e-dc5c-45c0-a4f4-7f63536cab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projection of e2 onto U is: [ 0.5  0.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "P_B = U @ np.linalg.inv(U.T @ B @ U) @ U.T @ B\n",
    "e2 = np.array([0, 1, 0])\n",
    "pe2 = P_B @ e2\n",
    "print(f\"The projection of e2 onto U is: {pe2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7d39d-1de2-400d-9b44-e87fff71bd73",
   "metadata": {},
   "source": [
    "b. *Compute \\\\(d(e_2, U)\\\\)*. \\\n",
    "We need \\\\(\\lVert e_2 - p_{e_2} \\rVert_B\\\\), the \\\\(B\\\\)-norm: for \\\\(r = e_2 - p_{e_2}\\\\), it's: \\\\(\\sqrt{r^TBr}\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "348707e6-8d80-4926-986e-107f2ce77b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The residual vector is: [-0.5  1.   0.5]\n",
      "The distance from e2 to U is: 1.0\n"
     ]
    }
   ],
   "source": [
    "r_e2 = e2 - pe2\n",
    "print(f\"The residual vector is: {r_e2}\")\n",
    "d_e2 = np.sqrt(r_e2.T @ B @ r_e2)\n",
    "print(f\"The distance from e2 to U is: {d_e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920548c-41a5-4758-927c-4b82ba52554f",
   "metadata": {},
   "source": [
    "#### Exercise 3.7:\n",
    "\n",
    "Let \\\\(V\\\\) be a vector space and \\\\(\\pi\\\\) an endomorphism (\\\\(V\\to V\\\\) mapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a4943-a8d6-43f1-a644-6929b38c2497",
   "metadata": {},
   "source": [
    "a. *Prove that \\\\(\\pi\\\\) is a projection iff \\\\(\\text{id}_V - \\pi\\\\) is a projection, where \\\\(\\text{id}_V\\\\) is the identity endomorphism on \\\\(V\\\\).* \n",
    "\n",
    "Approach and key insight: given that a projection is an idempotent mapping (i.e. \\\\(\\pi^2 = \\pi = \\pi \\circ \\pi\\\\)), we need to show that \\\\(\\pi\\\\) being idempotent implies the same for \\\\(\\text{id}_V\\\\), and then that the reverse is also true.\\\n",
    "Step 1: establish \\\\(\\rightarrow\\\\). \\\n",
    "Step 1a: if \\\\(\\text{id}_V - \\pi\\\\) is a projection, then it is idempotent, so: \\\\(\\text{id}_V - \\pi = (\\text{id}_V - \\pi)^2\\\\). \\\n",
    "Step 1b: expand the RHS: \\\\(\\text{id}_V - \\pi = \\text{id}_V - 2 \\text{id}_V\\pi + \\pi^2\\\\). Note that \\\\(\\text{id}_V\\\\) is itself idempotent, so we simplify by removing the square. \\\n",
    "Step 1c: in assuming it's a projection (since we are proving \\\\(\\rightarrow\\\\), we assume \\\\(\\pi\\\\) is idempotent. Then: \\\\(\\text{id}_V - \\pi = \\text{id}_V^2 - 2\\pi + \\pi\\\\). \\\n",
    "Step 1d: Simplify to \\\\(\\text{id}_V = \\text{id}_V\\\\). And we're done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c93df-b702-427e-880f-ebf43b811e2c",
   "metadata": {},
   "source": [
    "Step 2: establish \\\\(\\leftarrow\\\\). \\\n",
    "Step 2a: if \\\\(\\pi\\\\) is a projection, then it is idempotent, so need to prove: \\\\(\\pi = \\pi^2\\\\). \\\n",
    "Step 2b: given the same expansion from 1b earlier: \\\\(\\text{id}_V - \\pi = \\text{id}_V - 2 \\text{id}_V\\pi + \\pi^2\\\\). \\\n",
    "Step 2c: then (subtracting \\\\(\\text{id}_V\\\\) and adding \\\\(2\\pi\\\\) to both sides), we get: \\\\(\\pi = \\pi^2\\\\). And we're done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060504d6-0d40-4aa4-a889-5d22737ef290",
   "metadata": {},
   "source": [
    "b. *Assume now that \\\\(\\pi\\\\) is a projection. Calculate \\\\(\\text{Im}(\\text{id}_V - \\pi), \\text{ker}(\\text{id}_V - \\pi)\\\\) as functions of \\\\(\\text{Im}(\\pi), \\text{ker}(\\pi)\\\\).*\n",
    "\n",
    "Approach and key insight: an arbitrary vector \\\\(v\\in V = \\pi(v)\\in\\text{Im}(\\pi) + (v - \\pi(v))\\in\\text{ker}(\\pi)\\\\). \\\n",
    "Step 1: calculate \\\\((\\text{id}_V - \\pi)(v) = (\\text{id}_V - \\pi)(\\pi(u))\\\\) for some \\\\(u\\in V\\\\) s.t. \\\\(v = \\pi(u)\\\\) (where the identity is just from the definition of \\\\(v\\\\)). \\\n",
    "Step 1a: plug in \\\\(\\pi(u)\\\\) for \\\\(v\\\\): \\\\((\\text{id}_V - \\pi)(\\pi(u)) = (\\text{id}_V - \\pi)(\\pi(u))\\\\). \\\n",
    "Step 1b: simplify by distributing: \\\\(\\pi(u) - \\pi(\\pi(u)) = \\pi(u) - \\pi(\\pi(u)) = 0\\\\). \\\n",
    "Step 1c: So anything in \\\\(\\text{Im}(\\pi)\\\\) is also in \\\\(\\text{ker}(\\text{id}_V - \\pi)\\\\), i.e. \\\\(\\text{Im}(\\pi)\\subseteq\\text{ker}(\\text{id}_V - \\pi)\\\\) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bc649-db33-442a-977e-7d3805b6354a",
   "metadata": {},
   "source": [
    "Step 2: consider \\\\(v\\in\\text{ker}(\\pi)\\\\) s.t. \\\\(\\pi(v)=0\\\\). \\\n",
    "Step 2a: we now calculate \\\\((\\text{id}_V - \\pi)\\\\) given that \\\\(v\\\\). \\\n",
    "Step 2b: now we know that \\\\((\\text{id}_V - \\pi)(v) = v - \\pi(v) = v - 0 = v\\\\). \\\n",
    "Step 2c: so \\\\(\\text{ker}(\\pi)\\subseteq\\text{Im}(\\text{id}_V - \\pi)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05073d-b56c-4085-a423-3850f8c9714c",
   "metadata": {},
   "source": [
    "Step 3: now we need to show the other direction, and we'll start with showing \\\\(\\text{Im}(\\text{id}_V - \\pi)\\subseteq\\text{ker}(\\pi)\\\\). We know that (as above) \\\\(v = (\\text{id}_V - \\pi)(\\pi)\\in\\text{Im}(\\text{id}_V - \\pi) + (v - (\\text{id}_V - \\pi)(v))\\in\\text{ker}(\\text{id}_V - \\pi)\\\\). \\\n",
    "Step 3a: take some \\\\(u\\in V\\\\) s.t. \\\\(v = (\\text{id}_V - \\pi)(u)\\\\). So \\\\(\\pi(v) = \\pi((\\text{id}_V - \\pi)(u))\\\\). \\\n",
    "Step 3b: \\\\(\\pi((\\text{id}_V - \\pi)(u)) = (\\pi - \\pi^2)(u) = \\pi(u) - \\pi^2(u) = 0\\\\).\n",
    "Step 3c: we showed that a vector from the image of \\\\((\\text{id}_V - \\pi)\\\\) is mapped to \\\\(0\\\\) by \\\\(\\pi\\\\), i.e. \\\\(\\text{Im}(\\text{id}_V - \\pi)\\subseteq\\text{ker}(\\pi)\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e1187-2bdb-47b2-9e69-91f3acec68eb",
   "metadata": {},
   "source": [
    "Step 4: finally, we show that \\\\(\\ker(\\text{id}_V - \\pi)\\subseteq\\text{Im}(\\pi)\\\\). \\\n",
    "Step 4a: consider \\\\(v\\in\\ker(\\text{id}_V - \\pi)\\\\) s.t. \\\\((\\text{id}_V - \\pi)(v) = 0\\\\). \\\n",
    "Step 4b: we calculate \\\\(\\pi\\\\) given that \\\\(v\\\\). \\\n",
    "Step 4c: now we know that \\\\((v - (\\text{id}_V - \\pi)(v)) = v - (v - \\pi(v)) = \\pi(v) \\\\). \\\n",
    "Step 4d: so \\\\(\\ker(\\text{id}_V - \\pi)\\subseteq\\text{Im}(\\pi)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4105e1-cff4-4f5b-aee2-b7a36743d8a7",
   "metadata": {},
   "source": [
    "#### Exercise 3.8\n",
    "\n",
    "Using Gram-Schmidt basis \\\\(B = \\begin{bmatrix}b_1 & b_2\\end{bmatrix}\\\\) into an orthonormal basis \\\\(C\\\\), where:\n",
    "\\\\[\n",
    "b_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}, b_2 = \\begin{bmatrix}-1 \\\\ 2 \\\\ 0\\end{bmatrix}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b97679ec-a256-492e-91a7-ae898ef4fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This assumes that for an m-dimensional subspace in R^n, b is m x n.\n",
    "    \"\"\"\n",
    "    print(f\"b shape: {b.shape}\")\n",
    "    ortho_vecs = []\n",
    "\n",
    "    for v in b:\n",
    "        u = v\n",
    "        for prev_u in ortho_vecs:\n",
    "            u = u - np.dot(u, prev_u) / np.dot(prev_u, prev_u) * prev_u\n",
    "        if np.linalg.norm(u) > 1.0e-9:\n",
    "            ortho_vecs.append(u)\n",
    "    # normalize\n",
    "    ortho_vecs = [v / np.linalg.norm(v) for v in ortho_vecs]\n",
    "    return np.array(ortho_vecs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48b8f7be-50f9-41e8-9e70-73465ec5f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b shape: (2, 3)\n",
      "The new orthonormal basis is: [[ 0.57735027  0.57735027  0.57735027]\n",
      " [-0.6172134   0.77151675 -0.15430335]] of shape: (2, 3)\n",
      "CTC: [[1.00000000e+00 8.32361346e-17]\n",
      " [8.32361346e-17 1.00000000e+00]]\n",
      "C is orthogonal: True\n"
     ]
    }
   ],
   "source": [
    "b1 = np.array([1, 1, 1])\n",
    "b2 = np.array([-1, 2, 0])\n",
    "B = np.array([b1, b2])\n",
    "C = gram_schmidt(B)\n",
    "print(f\"The new orthonormal basis is: {C} of shape: {C.shape}\")\n",
    "eyes = C @ C.T # because it returns the new basis as rows rather than columns\n",
    "print(f\"CTC: {eyes}\")\n",
    "print(f\"C is orthogonal: {np.allclose(eyes, np.eye(C.shape[0]))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a950df-ecd3-4af5-8c6e-39060f7b83fa",
   "metadata": {},
   "source": [
    "#### Exercise 3.9\n",
    "\n",
    "Let \\\\(n\\in\\mathbb{N}\\\\) and \\\\(x_1, \\ldots, x_n > 0\\\\) be positive real numbers s.t. \\\\(\\sum_{i=1}^n x_i = 1\\\\). Use the Cauchy-Schwarz inequality  (\\\\(|\\langle x, y \\rangle|^2\\leq \\langle x, x\\rangle \\cdot \\langle y, y\\rangle\\\\)) to prove:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f75f5-027a-43c7-b40b-d44551513816",
   "metadata": {},
   "source": [
    "a. \\\\( \\sum_{i=1}^n x_i^2\\geq\\frac{1}{n}\\\\)\n",
    "\n",
    "Step 1: we know that we can decompose \\\\(\\sum_{i=1}^n x_i = 1\\\\) into \\\\(\\vec{x}\\cdot\\vec{y}\\\\) s.t. \\\\(y = \\begin{bmatrix}1 & \\cdots & 1\\end{bmatrix}\\\\), i.e. the dot product between the \\\\(x\\\\) and all-\\\\(1\\\\) vectors. \\\n",
    "Step 2: Thus \\\\(\\langle x, x\\rangle = \\sum_{i=1}^n x_i^2\\\\), and \\\\(\\langle y, y\\rangle = n\\\\). \\\n",
    "Step 3: So \\\\(\\langle x, x\\rangle \\cdot \\langle y, y\\rangle = \\sum_{i=1}^n x_i^2 \\cdot n\\\\) \\\n",
    "Step 4: \\\\(\\langle x, y\\rangle = \\sum_{i=1}^n (x_i\\cdot 1) = 1\\\\). \\\n",
    "Step 5: From Cauchy-Schwarz: \\\\(1 \\leq n\\sum_{i=1}^n x_i^2\\\\), so \\\\(\\frac{1}{n} \\leq \\sum_{i=1}^n x_i^2 \\\\). And we're done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44380b4-65e6-49dc-a4f1-b56d26317e95",
   "metadata": {},
   "source": [
    "b. \\\\(\\sum_{i=1}^n\\frac{1}{x_i}\\geq n^2\\\\)\n",
    "\n",
    "Step 1: select two vectors \\\\(u = \\sqrt{x_1}, \\ldots, \\sqrt{x_n}; v = \\frac{1}{\\sqrt{x_1}}, \\ldots, \\frac{1}{\\sqrt{x_n}}\\\\). \\\n",
    "Step 2: \\\\(\\langle u, u\\rangle = \\sum_{i=1}^n \\left(\\sqrt{x_i}\\right)^2 = 1\\\\) \\\n",
    "Step 3: \\\\(\\langle v, v\\rangle = \\sum_{i=1}^n \\left(\\frac{1}{\\sqrt{x_i}}\\right)^2 = \\sum_{i=1}^n \\frac{1}{x_i}\\\\) \\\n",
    "Step 4: \\\\(\\langle u, v\\rangle = \\sum_{i=1}^n \\frac{\\sqrt{x_i}}{\\sqrt{x_i}} = \\sum_{i=1}^n 1 = n\\\\) \\\n",
    "Step 5: \\\\(\\langle u, v\\rangle^2 = n^2\\\\) \\\n",
    "Step 6: \\\\(n^2\\leq 1\\cdot \\sum_{i=1}^n \\frac{1}{x_i}. And we're done.\\\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed0513b-d4d2-47d7-84f3-70c510d6101c",
   "metadata": {},
   "source": [
    "#### Exercise 3.10:\n",
    "\n",
    "Rotate by \\\\(30^{\\circ}\\\\):\n",
    "\\\\[\n",
    "x_1 = \\begin{bmatrix}2\\\\3\\end{bmatrix}, x_2 = \\begin{bmatrix}0 \\\\-1\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "We will assume these are in the standard basis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eaf61ed-c336-40cf-8252-f8b23c3fd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new x1, x2 coordinates are: [[ 0.23205081  0.5       ]\n",
      " [ 3.59807621 -0.8660254 ]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.pi / 6.0\n",
    "sin_theta = np.sin(theta)\n",
    "cos_theta = np.cos(theta)\n",
    "R_theta = np.array(\n",
    "    [\n",
    "        [cos_theta, -sin_theta],\n",
    "        [sin_theta, cos_theta]\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [2, 0],\n",
    "        [3, -1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "rotated = R_theta @ X\n",
    "print(f\"The new x1, x2 coordinates are: {rotated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62656f-5d25-47cd-abaf-c4bf1efa69ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
